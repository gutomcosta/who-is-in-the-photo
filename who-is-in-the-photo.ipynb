{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Nanodegree - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is in the photo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposal of this project is to build a model that allow to recognize persons in the image using Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 total person names.\n",
      "There are 2803 total face images.\n",
      "\n",
      "There are 2245 training face images.\n",
      "There are 277 validation face images.\n",
      "There are 281 test face images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def load_face_dataset(path):\n",
    "    data = load_files(path)\n",
    "    general_targets = data['target']\n",
    "    face_files = np.array(data['filenames'])\n",
    "    face_targets = np_utils.to_categorical(np.array(data['target']), 16)\n",
    "    return face_files, face_targets\n",
    "\n",
    "train_files, train_targets = load_face_dataset('faces2/train')\n",
    "valid_files, valid_targets = load_face_dataset('faces2/valid')\n",
    "test_files, test_targets = load_face_dataset('faces2/test')\n",
    "\n",
    "person_names = [item[12:-1] for item in sorted(glob(\"faces2/train/*/\"))]\n",
    "\n",
    "\n",
    "print('There are %d total person names.' % len(person_names))\n",
    "print('There are %s total face images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training face images.' % len(train_files))\n",
    "print('There are %d validation face images.' % len(valid_files))\n",
    "print('There are %d test face images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading image data into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(100, 100))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Image for baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def load_image_from(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img,(100,100))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def load_images_from(img_paths):\n",
    "    images = [load_image_from(path) for path in tqdm(img_paths)]\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load tensors to train, test and validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2245/2245 [00:00<00:00, 3075.07it/s]\n",
      "100%|██████████| 277/277 [00:00<00:00, 3656.32it/s]\n",
      "100%|██████████| 281/281 [00:00<00:00, 3589.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load image to train, test and validations of a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2245/2245 [00:00<00:00, 5836.67it/s]\n",
      "100%|██████████| 281/281 [00:00<00:00, 7174.25it/s]\n",
      "100%|██████████| 277/277 [00:00<00:00, 6948.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# base model\n",
    "base_train_images = load_images_from(train_files).astype('float32')/255\n",
    "base_test_images = load_images_from(test_files).astype('float32')/255\n",
    "base_valid_images = load_images_from(valid_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Train a base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_table(targets):\n",
    "    lookup = []\n",
    "    for target in targets:\n",
    "        index = np.where(target==1)[0]\n",
    "        lookup.append(index[0])\n",
    "    return np.array(lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_targets = create_lookup_table(train_targets)\n",
    "base_test_targets = create_lookup_table(test_targets)\n",
    "base_valid_targets = create_lookup_table(valid_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function to execute a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pca(train, test, n_components=16):\n",
    "    train = train[:,:,0]\n",
    "    test = test[:,:,0]\n",
    "    print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "        % (n_components, train.shape[0]))\n",
    "    t0 = time()\n",
    "    pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "            whiten=True).fit(train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    # eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "    print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "    t0 = time()\n",
    "    X_train_pca = pca.transform(train)\n",
    "    X_test_pca = pca.transform(test)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    return X_train_pca, X_test_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function to training a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svm(X_train, y_train):\n",
    "    # Train a SVM classification model\n",
    "    print \"Fitting the classifier to the training set\"\n",
    "    param_grid = {\n",
    "    'C': [1, 5, 10, 50, 100],\n",
    "    'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "    }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    print \"Best estimator found by grid search:\"\n",
    "    print clf.best_estimator_\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 16 eigenfaces from 2245 faces\n",
      "done in 0.090s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.005s\n",
      "Fitting the classifier to the training set\n"
     ]
    }
   ],
   "source": [
    "X_train = base_train_images\n",
    "y_train = base_train_targets\n",
    "\n",
    "X_test  = base_test_images\n",
    "y_test  = base_test_targets\n",
    "\n",
    "X_train_pca, X_test_pca = execute_pca(X_train,X_test)\n",
    "clf = apply_svm(X_train_pca, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    /Adam Sandler       0.00      0.00      0.00        11\n",
      "    /Alec Baldwin       1.00      0.10      0.18        10\n",
      "  /Angelina Jolie       0.14      0.05      0.07        21\n",
      " /Anna Kournikova       0.00      0.00      0.00        17\n",
      "  /Ashton Kutcher       0.00      0.00      0.00        10\n",
      "   /Avril Lavigne       0.10      0.50      0.16        30\n",
      "    /Barack Obama       0.14      0.26      0.18        27\n",
      "     /Ben Affleck       0.00      0.00      0.00        12\n",
      " /Beyonce Knowles       0.00      0.00      0.00        13\n",
      "       /Brad Pitt       0.17      0.10      0.12        30\n",
      "    /Cameron Diaz       0.20      0.08      0.11        25\n",
      "  /Cate Blanchett       0.00      0.00      0.00        16\n",
      " /Charlize Theron       0.00      0.00      0.00        20\n",
      " /Christina Ricci       0.00      0.00      0.00        14\n",
      "/Claudia Schiffer       0.09      0.08      0.09        12\n",
      "      /Clive Owen       0.00      0.00      0.00        13\n",
      "\n",
      "      avg / total       0.11      0.11      0.07       281\n",
      "\n",
      "[[ 0  0  0  0  0  6  2  0  0  1  0  0  1  0  1  0]\n",
      " [ 0  1  0  0  0  7  1  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0 12  5  0  0  0  1  0  0  0  2  0]\n",
      " [ 0  0  0  0  0  8  4  1  0  0  1  0  1  0  2  0]\n",
      " [ 0  0  0  0  0  5  1  0  0  2  1  0  0  0  0  1]\n",
      " [ 0  0  0  0  0 15  4  1  0  4  1  0  2  2  1  0]\n",
      " [ 0  0  0  1  0 18  7  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  7  0  0  0  1  1  0  1  0  1  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  2  0  0 12  8  1  0  3  1  0  1  1  1  0]\n",
      " [ 0  0  0  0  1 16  3  0  1  2  2  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  6  4  1  0  1  0  0  2  0  0  0]\n",
      " [ 0  0  0  1  0 12  2  0  0  0  2  1  0  0  1  1]\n",
      " [ 0  0  3  0  0  8  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  7  1  0  0  0  0  0  0  2  1  0]\n",
      " [ 0  0  0  0  0  5  5  0  0  2  0  0  0  1  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luiz/.virtualenvs/cv/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_pca)\n",
    "print classification_report(y_test, y_pred, target_names=person_names)\n",
    "\n",
    "print confusion_matrix(y_test, y_pred, labels=range(len(person_names)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
